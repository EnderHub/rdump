# Story 1.16: Create Streaming Example Program

## Status
Draft

## Story
**As a** library user,
**I want** a comprehensive example demonstrating the streaming API,
**so that** I can learn memory-efficient patterns for processing large codebases.

## Acceptance Criteria

**Compilation:**
1. - [ ] Example compiles without warnings
2. - [ ] Example runs successfully
3. - [ ] Works with `cargo run --example streaming_search`

**Content:**
4. - [ ] Shows basic streaming iteration
5. - [ ] Shows early termination with `.take()`
6. - [ ] Shows error handling strategies (collect, skip, fail-fast)
7. - [ ] Shows progress reporting during iteration
8. - [ ] Shows parallel processing pattern
9. - [ ] Shows memory-efficient aggregation

**Code Quality:**
10. - [ ] Includes helpful comments explaining patterns
11. - [ ] Shows practical real-world use cases
12. - [ ] Demonstrates memory efficiency benefits
13. - [ ] Additional patterns for reference

**Output:**
14. - [ ] Produces clear, informative output
15. - [ ] Shows timing and statistics
16. - [ ] Handles edge cases (empty results)

## Tasks / Subtasks

- [ ] Create example file `examples/streaming_search.rs` (AC: 1, 2, 3)
- [ ] Implement example functions (AC: 4-9)
  - [ ] `example_basic_streaming()` - process one at a time
  - [ ] `example_early_termination()` - stop after N results
  - [ ] `example_skip_errors()` - error handling strategies
  - [ ] `example_progress_reporting()` - show progress
  - [ ] `example_parallel_processing()` - rayon integration
  - [ ] `example_memory_efficient_aggregation()` - statistics without storing
- [ ] Add additional patterns (AC: 13)
  - [ ] Find first with condition
  - [ ] Windowed/batch processing
  - [ ] Two-phase processing
  - [ ] Streaming to external system
- [ ] Add comments and documentation (AC: 10, 11, 12)
- [ ] Ensure good output (AC: 14, 15, 16)

## Dev Notes

### Location
- Example file: `examples/streaming_search.rs`
- [Source: docs/epics/library-api-epic.md - Story 16]

### Why Streaming Matters
| Scenario | Collect All | Streaming |
|----------|-------------|-----------|
| 10K files, take 10 | Load all 10K | Load only 10 |
| 1K files, count total | ~100 MB | ~10 KB peak |
| Find first match | Load until found | Stop immediately |

### Running the Example
```bash
# Run in project directory
cargo run --example streaming_search

# Run against a large codebase
cd /path/to/large/project
cargo run --example streaming_search --manifest-path /path/to/rdump/Cargo.toml
```

### Dependencies
This example uses rayon for parallel processing:
```toml
[dev-dependencies]
rayon = "1"
```

### Complete Implementation

```rust
//! Streaming search example for rdump library
//!
//! Demonstrates memory-efficient patterns for processing large codebases
//! using the search_iter() API.
//!
//! Run with: cargo run --example streaming_search

use anyhow::Result;
use rdump::{search_iter, SearchOptions, SearchResult};
use std::io::{self, Write};
use std::time::Instant;

fn main() -> Result<()> {
    println!("rdump Streaming API Examples\n");

    // Example 1: Basic streaming iteration
    example_basic_streaming()?;

    // Example 2: Early termination
    example_early_termination()?;

    // Example 3: Skip errors and continue
    example_skip_errors()?;

    // Example 4: Progress reporting
    example_progress_reporting()?;

    // Example 5: Parallel processing of results
    example_parallel_processing()?;

    // Example 6: Memory-efficient aggregation
    example_memory_efficient_aggregation()?;

    println!("\nAll streaming examples completed!");
    Ok(())
}

/// Example 1: Basic streaming iteration
///
/// Process results one at a time without loading all into memory
fn example_basic_streaming() -> Result<()> {
    println!("=== Example 1: Basic Streaming ===");

    let iter = search_iter("ext:rs", SearchOptions::default())?;

    println!("Processing {} files...", iter.remaining());

    let mut count = 0;
    for result in iter {
        match result {
            Ok(r) => {
                count += 1;
                // Process each file individually
                // Content is loaded only when we reach this point
                if count <= 3 {
                    println!("  {} ({} bytes)", r.path.display(), r.content.len());
                }
            }
            Err(e) => {
                eprintln!("  Error: {}", e);
            }
        }
    }

    if count > 3 {
        println!("  ... and {} more files", count - 3);
    }

    println!();
    Ok(())
}

/// Example 2: Early termination with .take()
///
/// Stop processing after finding N results - remaining files are never read
fn example_early_termination() -> Result<()> {
    println!("=== Example 2: Early Termination ===");

    let iter = search_iter("ext:rs", SearchOptions::default())?;
    let total = iter.remaining();

    println!("Total matching files: {}", total);
    println!("Taking only first 5...\n");

    // Only the first 5 files will have their content read
    let first_five: Vec<SearchResult> = iter
        .take(5)
        .filter_map(Result::ok)
        .collect();

    for result in &first_five {
        println!("  - {}", result.path.display());
    }

    println!("\nProcessed {} of {} files", first_five.len(), total);
    println!("(Remaining {} files were not read from disk)\n", total - first_five.len());

    Ok(())
}

/// Example 3: Skip errors and continue processing
///
/// Demonstrates error handling strategies
fn example_skip_errors() -> Result<()> {
    println!("=== Example 3: Error Handling ===");

    let iter = search_iter("ext:rs", SearchOptions::default())?;

    // Strategy 1: Collect all errors
    let mut successes = Vec::new();
    let mut errors = Vec::new();

    for result in iter {
        match result {
            Ok(r) => successes.push(r),
            Err(e) => errors.push(e),
        }
    }

    println!("Successes: {}, Errors: {}", successes.len(), errors.len());

    // Strategy 2: Skip errors silently (use filter_map)
    let iter2 = search_iter("ext:rs", SearchOptions::default())?;
    let results: Vec<_> = iter2.filter_map(Result::ok).collect();
    println!("Results (skipping errors): {}", results.len());

    // Strategy 3: Fail on first error (use collect::<Result<Vec<_>, _>>())
    let iter3 = search_iter("ext:rs", SearchOptions::default())?;
    match iter3.collect::<Result<Vec<_>, _>>() {
        Ok(results) => println!("All {} files processed successfully", results.len()),
        Err(e) => println!("Failed on: {}", e),
    }

    println!();
    Ok(())
}

/// Example 4: Progress reporting during iteration
fn example_progress_reporting() -> Result<()> {
    println!("=== Example 4: Progress Reporting ===");

    let mut iter = search_iter("ext:rs", SearchOptions::default())?;
    let total = iter.remaining();

    if total == 0 {
        println!("No files to process\n");
        return Ok(());
    }

    let start = Instant::now();
    let mut processed = 0;
    let mut total_bytes = 0;

    print!("Processing: ");
    io::stdout().flush()?;

    while let Some(result) = iter.next() {
        processed += 1;

        if let Ok(r) = result {
            total_bytes += r.content.len();
        }

        // Update progress every 10 files
        if processed % 10 == 0 || processed == total {
            print!("\rProcessing: {}/{} files ({:.1}%)",
                processed, total,
                (processed as f64 / total as f64) * 100.0
            );
            io::stdout().flush()?;
        }
    }

    let elapsed = start.elapsed();
    println!("\n\nCompleted in {:?}", elapsed);
    println!("Total bytes processed: {} KB", total_bytes / 1024);
    println!("Average: {:.2} files/sec\n", processed as f64 / elapsed.as_secs_f64());

    Ok(())
}

/// Example 5: Parallel processing of streamed results
///
/// Collect results then process them in parallel
fn example_parallel_processing() -> Result<()> {
    println!("=== Example 5: Parallel Processing ===");

    use rayon::prelude::*;

    // First, collect results (streaming)
    let results: Vec<_> = search_iter("ext:rs", SearchOptions::default())?
        .filter_map(Result::ok)
        .collect();

    println!("Collected {} files", results.len());

    // Then process in parallel
    let total_lines: usize = results
        .par_iter()
        .map(|r| r.content.lines().count())
        .sum();

    println!("Total lines of code: {}", total_lines);

    // Find file with most lines (parallel)
    if let Some((path, lines)) = results
        .par_iter()
        .map(|r| (&r.path, r.content.lines().count()))
        .max_by_key(|(_, lines)| *lines)
    {
        println!("Largest file: {} ({} lines)", path.display(), lines);
    }

    println!();
    Ok(())
}

/// Example 6: Memory-efficient aggregation
///
/// Aggregate statistics without keeping all results in memory
fn example_memory_efficient_aggregation() -> Result<()> {
    println!("=== Example 6: Memory-Efficient Aggregation ===");

    let iter = search_iter("func:*", SearchOptions {
        presets: vec!["rust".to_string()],
        ..Default::default()
    })?;

    // Track statistics without storing all results
    let mut file_count = 0;
    let mut total_functions = 0;
    let mut max_functions = 0;
    let mut max_functions_file = String::new();
    let mut total_lines = 0;

    for result in iter.filter_map(Result::ok) {
        file_count += 1;
        let func_count = result.match_count();
        total_functions += func_count;

        if func_count > max_functions {
            max_functions = func_count;
            max_functions_file = result.path.display().to_string();
        }

        // Count lines in matched functions
        for m in &result.matches {
            total_lines += m.line_count();
        }

        // result is dropped here - content memory is freed
    }

    println!("Statistics:");
    println!("  Files analyzed: {}", file_count);
    println!("  Total functions: {}", total_functions);
    println!("  Total function lines: {}", total_lines);
    if file_count > 0 {
        println!("  Avg functions/file: {:.1}", total_functions as f64 / file_count as f64);
        println!("  Max functions in file: {} ({})", max_functions, max_functions_file);
    }

    println!();
    Ok(())
}

// =============================================================================
// Additional streaming patterns
// =============================================================================

/// Pattern: Find first match meeting criteria
#[allow(dead_code)]
fn find_first_with_condition() -> Result<Option<SearchResult>> {
    let iter = search_iter("ext:rs", SearchOptions::default())?;

    // Find first file containing "unsafe"
    for result in iter {
        if let Ok(r) = result {
            if r.content.contains("unsafe") {
                return Ok(Some(r));
            }
        }
    }

    Ok(None)
}

/// Pattern: Windowed processing
#[allow(dead_code)]
fn process_in_batches() -> Result<()> {
    let mut iter = search_iter("ext:rs", SearchOptions::default())?;

    let batch_size = 100;
    let mut batch = Vec::with_capacity(batch_size);

    loop {
        // Fill batch
        batch.clear();
        for _ in 0..batch_size {
            match iter.next() {
                Some(Ok(r)) => batch.push(r),
                Some(Err(_)) => continue,
                None => break,
            }
        }

        if batch.is_empty() {
            break;
        }

        // Process batch
        println!("Processing batch of {} files", batch.len());
        // ... batch processing logic ...
    }

    Ok(())
}

/// Pattern: Two-phase processing
#[allow(dead_code)]
fn two_phase_processing() -> Result<()> {
    // Phase 1: Quick scan to collect paths
    let paths: Vec<_> = search_iter("ext:rs", SearchOptions::default())?
        .filter_map(Result::ok)
        .map(|r| r.path.clone())
        .collect();

    println!("Phase 1: Found {} files", paths.len());

    // Phase 2: Detailed analysis (could be done in parallel or on-demand)
    for path in paths.iter().take(10) {
        // Re-read specific files for detailed analysis
        let content = std::fs::read_to_string(path)?;
        println!("Analyzing: {} ({} bytes)", path.display(), content.len());
    }

    Ok(())
}

/// Pattern: Streaming to external system
#[allow(dead_code)]
fn stream_to_external() -> Result<()> {
    let iter = search_iter("ext:rs", SearchOptions::default())?;

    for result in iter.filter_map(Result::ok) {
        // Stream each result to external system immediately
        // without accumulating in memory
        send_to_database(&result)?;
        send_to_search_index(&result)?;
    }

    Ok(())
}

fn send_to_database(_result: &SearchResult) -> Result<()> {
    // Placeholder for database insertion
    Ok(())
}

fn send_to_search_index(_result: &SearchResult) -> Result<()> {
    // Placeholder for search index update
    Ok(())
}
```

### Testing

**Test Location:** Example runs as integration test

**Verification:**
- `cargo run --example streaming_search` completes successfully
- Shows timing and statistics
- Memory-efficient patterns work correctly
- [Source: docs/architecture.md#test-strategy]

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-21 | 0.1 | Initial draft | SM Agent |
